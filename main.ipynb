{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2fa6da",
   "metadata": {},
   "source": [
    "# tinygrad – minimalistyczna biblioteka głębokiego uczenia maszynowego\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "tinygrad to lekka, otwarta biblioteka stworzona przez George'a Hotza (tiny corp). Łączy **prostotę** mikrobiblioteki _micrograd_ Karpathy'ego z funkcjonalnością podobną do PyTorch'a. Ze względu na niewielki (10_000 linii), czytelny kod źródłowy jest polecana początkującym, którzy chcą zrozumieć wewnętrzne mechanizmy sieci neuronowych. Biblioteka jest w fazie alpha, ale zyskuje popularność (np. w projekcie OpenPilot jako backend GPU).\n",
    "\n",
    "---\n",
    "\n",
    "- micrograd to 'biblioteka', który implementuje zwykłą sieć neuronową w 100 linijkach kodu w Pythonie i to tyle, nie jest już utrzymywana\n",
    "- Dla ludzi, którzy wiedzą jak trenować sieci neuronowe, ale chcą zrozumieć jak działają od strony implementacyjnej\n",
    "- OpenPilot to open source software do autonomicznych samochodów, stworzony przez firme comma.ai, założoną przez George'a Hotza\n",
    "\n",
    "---\n",
    "\n",
    "## Historia i kontekst powstania\n",
    "\n",
    "George Hotz stworzył tinygrad w 2020 roku jako alternatywę dla coraz bardziej skomplikowanych frameworków ML. Inspiracją był micrograd Andreja Karpathy'ego, ale z ambicją stworzenia biblioteki zdolnej do trenowania realnych modeli. Projekt wynika z filozofii tiny corp: \"najmniejsze narzędzie, które działa\". Hotz argumentuje, że współczesne biblioteki ML stały się zbyt złożone, co utrudnia innowacje i zrozumienie. Większość developerów nie potrzebuje wszystkich funkcji PyTorch'a, Tinygrad implementuje tylko niezbędne elementy.\n",
    "\n",
    "## Podstawowe koncepcje i składnia tinygrad\n",
    "\n",
    "Główną klasą jest `Tensor`, analogiczna do `torch.Tensor`. Na tensorach wykonujemy operacje element-po-elemencie (np. `x + y`), mnożenie macierzy (`x.matmul(y)`), stosujemy funkcje aktywacyjne (`.relu()`, `.sigmoid()`) oraz inne operacje (np. `reshape`, `permute`). Obliczenia są **leniwe** – wykonywane dopiero po poproszeniu o wynik, co pozwala tinygrad łączyć operacje w zoptymalizowane jądra obliczeniowe.\n",
    "\n",
    "tinygrad wspiera **autograd**: wystarczy utworzyć tensor z `requires_grad=True`, wykonać operacje i wywołać `loss.backward()`, aby automatycznie obliczyć gradienty:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4216fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = [ 5. 12. 21.] ∂d/∂a = [5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "c = a * b + a\n",
    "d = c.sum()\n",
    "d.backward()\n",
    "print(\"c =\", c.numpy(), \"∂d/∂a =\", a.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a57d5",
   "metadata": {},
   "source": [
    "## Porównanie tinygrad z PyTorch\n",
    "\n",
    "- **Interfejs (API)**: tinygrad celowo naśladuje PyTorch – składnia jest prawie identyczna, co ułatwia migrację:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d69783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tinygrad\n",
    "from tinygrad.tensor import Tensor\n",
    "x = Tensor.eye(3, requires_grad=True)\n",
    "y = Tensor([[2.0, 0, -2.0]], requires_grad=True)\n",
    "z = y.matmul(x).sum()\n",
    "z.backward()\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "x = torch.eye(3, requires_grad=True)\n",
    "y = torch.tensor([[2.0, 0, -2.0]], requires_grad=True)\n",
    "z = y.matmul(x).sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba5749",
   "metadata": {},
   "source": [
    "- **Poziom abstrakcji**: PyTorch jest rozbudowanym frameworkiem wysokiego poziomu, podczas gdy tinygrad stawia na prostotę. Tinygrad **nie ma klasy `nn.Module`** – zamiast tego model to zwykła klasa Pythona, a propagację definiuje się przez `__call__`. Zachęca to do funkcjonalnego stylu programowania.\n",
    "\n",
    "- **Wydajność**: PyTorch jest generalnie szybszy dzięki zoptymalizowanym bibliotekom. Tinygrad ma potencjalne zalety – może kompilować zestawy operacji do spersonalizowanych kerneli, a prostszy backend ułatwia optymalizację – ale obecnie nie dorównuje wydajnością.\n",
    "\n",
    "- **Cel i zastosowania**: PyTorch jest skierowany na projekty produkcyjne, a tinygrad – przede wszystkim na **naukę i eksperymenty**. Jest także używany w zastosowaniach wbudowanych, gdzie lekkość implementacji ma znaczenie.\n",
    "\n",
    "- **Implementacja nowych backendów**: Wiele firm tworzy obecnie własne akceleratory ML pod różne zastosowania (np. Google TPU, Apple Neural Engine, Groq, Graphcore). Aby PyTorch działał na tych akceleratorach, twórcy muszą zaimplementować około 200-300 operacji kernela. Dla TinyGrad jest to zaledwie 25-40 operacji.\n",
    "  Przykład: TinyGrad nie ma dedykowanego kernela Dropout - implementuje go złożeniem kilku prostych operacji (losowa maska + mnożenie). Choć TinyGrad może być przez to nieco wolniejszy, pozwala firmom na znacznie szybszą implementację wsparcia dla akceleratora, a później stopniową optymalizację krytycznych ścieżek.\n",
    "  Jest to analogiczne do architektury procesorów CISC (PyTorch - dużo wyspecjalizowanych instrukcji) vs RISC (TinyGrad - mała liczba uniwersalnych instrukcji), gdzie RISC upraszcza implementację procesora kosztem potencjalnie mniejszej wydajności niektórych operacji.\n",
    "\n",
    "## Dlaczego wybrać tinygrad?\n",
    "\n",
    "- **Łatwość zrozumienia**: tinygrad ma mały, czytelny kod źródłowy zawierający tylko niezbędne elementy.\n",
    "- **Eksperymenty z akceleratorami**: zaprojektowany tak, by łatwo dodawać nowe backendy – każdy nowy akcelerator musi zaimplementować tylko kilkanaście podstawowych operacji.\n",
    "- **Wsparcie GPU**: mimo prostoty, tinygrad może korzystać z GPU (OpenCL, CUDA, Triton) dla przyspieszenia obliczeń.\n",
    "- **Ograniczenia**: nie jest przeznaczony do dużych, produkcyjnych projektów i nie ma tak bogatej biblioteki funkcji jak PyTorch.\n",
    "\n",
    "## Architektura tinygrad\n",
    "\n",
    "Tinygrad rozkłada skomplikowane obliczenia na **trzy podstawowe typy operacji**: elementarne, redukujące oraz przesunięcia danych. Wszystkie wyższe funkcje są zbudowane z kompozycji tych prostych bloków.\n",
    "\n",
    "Obliczenia są wykonywane **leniwie**, co pozwala na fuzję wielu operacji w jeden program GPU/CPU. Dla przyspieszenia powtarzalnych fragmentów kodu, oferuje prosty JIT (dekorator `@TinyJit`).\n",
    "\n",
    "Korzysta z prostych backendów: CPU realizuje operacje przez NumPy/Python, a GPU – przez OpenCL, CUDA, METAL itp. Dzięki temu wspiera wiele akceleratorów bez złożonej implementacji.\n",
    "\n",
    "## Praktyczne zastosowanie – Klasyfikator MNIST\n",
    "\n",
    "Poniżej kompletny przykład trenowania klasyfikatora MNIST w tinygrad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba5d670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([60000, 1, 28, 28]) torch.float32 torch.Size([60000]) torch.int64\n",
      "step    0, loss 2.33, acc 15.90%\n",
      "step  100, loss 0.29, acc 82.94%\n",
      "step  200, loss 0.75, acc 89.55%\n",
      "step  300, loss 0.53, acc 91.50%\n",
      "step  400, loss 0.25, acc 92.70%\n",
      "step  500, loss 0.42, acc 93.80%\n",
      "step  600, loss 0.40, acc 94.98%\n",
      "step  700, loss 0.04, acc 95.14%\n",
      "step  800, loss 0.04, acc 95.55%\n",
      "step  900, loss 0.08, acc 95.90%\n",
      "step 1000, loss 0.01, acc 95.81%\n",
      "step 1100, loss 0.09, acc 96.10%\n",
      "step 1200, loss 0.01, acc 96.07%\n",
      "step 1300, loss 0.07, acc 96.49%\n",
      "step 1400, loss 1.00, acc 96.14%\n",
      "step 1500, loss 0.02, acc 96.77%\n",
      "step 1600, loss 0.01, acc 96.93%\n",
      "step 1700, loss 0.06, acc 97.10%\n",
      "step 1800, loss 0.02, acc 96.81%\n",
      "step 1900, loss 0.02, acc 96.73%\n",
      "total time 7.11711573600769\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from time import time\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.l1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.l2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.l3 = nn.Linear(1600, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.l1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.l2(x)), (2, 2))\n",
    "        x = x.flatten(1)\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        return self.l3(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = torch.stack([img for img, _ in train_dataset])\n",
    "Y_train = torch.tensor([label for _, label in train_dataset])\n",
    "X_test = torch.stack([img for img, _ in test_dataset])\n",
    "Y_test = torch.tensor([label for _, label in test_dataset])\n",
    "\n",
    "print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n",
    "\n",
    "model = MNISTClassifier().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time()\n",
    "batch_size = 8\n",
    "for step in range(2000):\n",
    "    indices = torch.randint(0, len(X_train), (batch_size,))\n",
    "    X, Y = X_train[indices].to(device), Y_train[indices].to(device)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = F.cross_entropy(outputs, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test.to(device))\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            acc = (predicted == Y_test.to(device)).float().mean().item()\n",
    "        print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100:.2f}%\")\n",
    "print(\"total time\", time()-start)\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead55ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n",
      "(60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n",
      "acc 0.1283\n",
      "acc 0.8502\n",
      "acc 0.8838\n",
      "acc 0.9171\n",
      "acc 0.8810\n",
      "acc 0.9078\n",
      "acc 0.9333\n",
      "acc 0.9148\n",
      "acc 0.9319\n",
      "acc 0.9385\n",
      "acc 0.9258\n",
      "acc 0.9365\n",
      "acc 0.9333\n",
      "acc 0.9456\n",
      "acc 0.9485\n",
      "acc 0.9486\n",
      "acc 0.9463\n",
      "acc 0.9426\n",
      "acc 0.9580\n",
      "acc 0.9446\n",
      "total time 31.083072900772095\n"
     ]
    }
   ],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "from tinygrad.nn.datasets import mnist\n",
    "from time import time\n",
    "\n",
    "\n",
    "class MNISTClassifier:\n",
    "  def __init__(self):\n",
    "    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n",
    "    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n",
    "    self.l3 = nn.Linear(1600, 10)\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor:\n",
    "    x = self.l1(x).relu().max_pool2d((2,2))\n",
    "    x = self.l2(x).relu().max_pool2d((2,2))\n",
    "    return self.l3(x.flatten(1).dropout(0.5))\n",
    "\n",
    "print(Device.DEFAULT)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n",
    "\n",
    "model = MNISTClassifier()\n",
    "acc = (model(X_test).argmax(axis=1) == Y_test).mean()\n",
    "# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\n",
    "\n",
    "optim = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "batch_size = 8\n",
    "\n",
    "@TinyJit\n",
    "def step():\n",
    "  Tensor.training = True  # makes dropout work\n",
    "  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "  X, Y = X_train[samples], Y_train[samples]\n",
    "  optim.zero_grad()\n",
    "  loss = model(X).sparse_categorical_crossentropy(Y).backward()\n",
    "  optim.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "start = time()\n",
    "for i in range(2000):\n",
    "  loss = step()\n",
    "  if i%100 == 0:\n",
    "    Tensor.training = False\n",
    "    acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n",
    "    print(f\"acc {acc:.4f}\")\n",
    "print(\"total time\", time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f49c4c",
   "metadata": {},
   "source": [
    "## Zastosowania w projektach rzeczywistych\n",
    "\n",
    "Przykłady wykorzystania tinygrad w praktyce:\n",
    "\n",
    "- OpenPilot (komponent samojezdnego samochodu)\n",
    "- Mobilne przetwarzanie obrazów\n",
    "- Projekty edukacyjne wymagające zrozumienia całego stosu ML\n",
    "\n",
    "## Najważniejsze różnice z kulturą PyTorch i TensorFlow\n",
    "\n",
    "PyTorch/TensorFlow promują używanie gotowych abstrakcji wysokiego poziomu (modele, warstwy). tinygrad zachęca do budowania wszystkiego od podstaw - rozumienie, a nie tylko użycie. Różnica filozoficzna to również podejście do debugowania - w tinygrad widać wszystkie operacje, podczas gdy w większych bibliotekach wiele operacji dzieje się \"pod maską\".\n",
    "\n",
    "W tinygrad możemy użyć `with Context(DEBUG=2)`, aby zobaczyć wszystkie kernele, które są wykonywane. DEBUG=4, pokazuje cały kod, w PyTorch jest to ciężkie.\n",
    "\n",
    "## Przyszłość tinygrad\n",
    "\n",
    "Tinycorp regularnie rozwija projekt, dodając:\n",
    "\n",
    "- Wsparcie dla nowych architektur (Apple M1/M2)\n",
    "- Optymalizacje kompilatora (JIT)\n",
    "- Wsparcie dla nowych modeli (np. LLM, stable diffusion)\n",
    "\n",
    "## Zasoby do dalszego zgłębienia\n",
    "\n",
    "- Oficjalne repozytorium: https://github.com/tinygrad/tinygrad\n",
    "- Kanał YouTube George'a Hotza: omówienia implementacji https://www.youtube.com/@geohotarchive/videos\n",
    "- Dokumentacja: https://docs.tinygrad.org/\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "tinygrad to **bardzo lekki** framework, którego głównym celem jest edukacja i zrozumienie działania głębokiego uczenia. Oferuje uproszczony interfejs podobny do PyTorch, ale rezygnuje z wielu abstrakcji. Najważniejsze zalety: prostota implementacji, czytelność kodu i łatwość rozbudowy. Stanowi interesującą alternatywę do nauki mechanizmów uczenia, pokazując „od podszewki\" jak działa propagacja wsteczna i przetwarzanie tensorów.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
