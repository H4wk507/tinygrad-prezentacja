{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2fa6da",
   "metadata": {},
   "source": [
    "# tinygrad – minimalistyczna biblioteka głębokiego uczenia maszynowego\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "tinygrad to lekka, otwarta biblioteka stworzona przez George'a Hotza (tiny corp). Łączy **prostotę** mikrobiblioteki _micrograd_ Karpathy'ego z funkcjonalnością podobną do PyTorch'a. Ze względu na niewielki (10_000 linii), czytelny kod źródłowy jest polecana początkującym, którzy chcą zrozumieć wewnętrzne mechanizmy sieci neuronowych. Biblioteka jest w fazie alpha, ale zyskuje popularność (np. w projekcie OpenPilot jako backend GPU).\n",
    "\n",
    "---\n",
    "\n",
    "- micrograd to 'biblioteka', który implementuje zwykłą sieć neuronową w 100 linijkach kodu w Pythonie i to tyle, nie jest już utrzymywana\n",
    "- Dla ludzi, którzy wiedzą jak trenować sieci neuronowe, ale chcą zrozumieć jak działają od strony implementacyjnej\n",
    "- OpenPilot to open source software do autonomicznych samochodów, stworzony przez firme comma.ai, założoną przez George'a Hotza\n",
    "\n",
    "---\n",
    "\n",
    "## Historia i kontekst powstania\n",
    "\n",
    "George Hotz stworzył tinygrad w 2020 roku jako alternatywę dla coraz bardziej skomplikowanych frameworków ML. Inspiracją był micrograd Andreja Karpathy'ego, ale z ambicją stworzenia biblioteki zdolnej do trenowania realnych modeli. Projekt wynika z filozofii tiny corp: \"najmniejsze narzędzie, które działa\". Hotz argumentuje, że współczesne biblioteki ML stały się zbyt złożone, co utrudnia innowacje i zrozumienie. Większość developerów nie potrzebuje wszystkich funkcji PyTorch'a, Tinygrad implementuje tylko niezbędne elementy.\n",
    "\n",
    "## Podstawowe koncepcje i składnia tinygrad\n",
    "\n",
    "Główną klasą jest `Tensor`, analogiczna do `torch.Tensor`. Na tensorach wykonujemy operacje element-po-elemencie (np. `x + y`), mnożenie macierzy (`x.matmul(y)`), stosujemy funkcje aktywacyjne (`.relu()`, `.sigmoid()`) oraz inne operacje (np. `reshape`, `permute`). Obliczenia są **leniwe** – wykonywane dopiero po poproszeniu o wynik, co pozwala tinygrad łączyć operacje w zoptymalizowane jądra obliczeniowe.\n",
    "\n",
    "tinygrad wspiera **autograd**: wystarczy utworzyć tensor z `requires_grad=True`, wykonać operacje i wywołać `loss.backward()`, aby automatycznie obliczyć gradienty:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4216fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "c = a * b + a\n",
    "d = c.sum()\n",
    "d.backward()\n",
    "print(\"c =\", c.numpy(), \"∂d/∂a =\", a.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a57d5",
   "metadata": {},
   "source": [
    "## Porównanie tinygrad z PyTorch\n",
    "\n",
    "- **Interfejs (API)**: tinygrad celowo naśladuje PyTorch – składnia jest prawie identyczna, co ułatwia migrację:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d69783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tinygrad\n",
    "from tinygrad.tensor import Tensor\n",
    "x = Tensor.eye(3, requires_grad=True)\n",
    "y = Tensor([[2.0, 0, -2.0]], requires_grad=True)\n",
    "z = y.matmul(x).sum()\n",
    "z.backward()\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "x = torch.eye(3, requires_grad=True)\n",
    "y = torch.tensor([[2.0, 0, -2.0]], requires_grad=True)\n",
    "z = y.matmul(x).sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba5749",
   "metadata": {},
   "source": [
    "- **Poziom abstrakcji**: PyTorch jest rozbudowanym frameworkiem wysokiego poziomu, podczas gdy tinygrad stawia na prostotę. Tinygrad **nie ma klasy `nn.Module`** – zamiast tego model to zwykła klasa Pythona, a propagację definiuje się przez `__call__`. Zachęca to do funkcjonalnego stylu programowania.\n",
    "\n",
    "- **Wydajność**: PyTorch jest generalnie szybszy dzięki zoptymalizowanym bibliotekom. Tinygrad ma potencjalne zalety – może kompilować zestawy operacji do spersonalizowanych kerneli, a prostszy backend ułatwia optymalizację – ale obecnie nie dorównuje wydajnością.\n",
    "\n",
    "- **Cel i zastosowania**: PyTorch jest skierowany na projekty produkcyjne, a tinygrad – przede wszystkim na **naukę i eksperymenty**. Jest także używany w zastosowaniach wbudowanych, gdzie lekkość implementacji ma znaczenie.\n",
    "\n",
    "- **Implementacja nowych backendów**: Wiele firm tworzy obecnie własne akceleratory ML pod różne zastosowania (np. Google TPU, Apple Neural Engine, Groq, Graphcore). Aby PyTorch działał na tych akceleratorach, twórcy muszą zaimplementować około 200-300 operacji kernela. Dla TinyGrad jest to zaledwie 25-40 operacji.\n",
    "  Przykład: TinyGrad nie ma dedykowanego kernela Dropout - implementuje go złożeniem kilku prostych operacji (losowa maska + mnożenie). Choć TinyGrad może być przez to nieco wolniejszy, pozwala firmom na znacznie szybszą implementację wsparcia dla akceleratora, a później stopniową optymalizację krytycznych ścieżek.\n",
    "  Jest to analogiczne do architektury procesorów CISC (PyTorch - dużo wyspecjalizowanych instrukcji) vs RISC (TinyGrad - mała liczba uniwersalnych instrukcji), gdzie RISC upraszcza implementację procesora kosztem potencjalnie mniejszej wydajności niektórych operacji.\n",
    "\n",
    "## Dlaczego wybrać tinygrad?\n",
    "\n",
    "- **Łatwość zrozumienia**: tinygrad ma mały, czytelny kod źródłowy zawierający tylko niezbędne elementy.\n",
    "- **Eksperymenty z akceleratorami**: zaprojektowany tak, by łatwo dodawać nowe backendy – każdy nowy akcelerator musi zaimplementować tylko kilkanaście podstawowych operacji.\n",
    "- **Wsparcie GPU**: mimo prostoty, tinygrad może korzystać z GPU (OpenCL, CUDA, Triton) dla przyspieszenia obliczeń.\n",
    "- **Ograniczenia**: nie jest przeznaczony do dużych, produkcyjnych projektów i nie ma tak bogatej biblioteki funkcji jak PyTorch.\n",
    "\n",
    "## Architektura tinygrad\n",
    "\n",
    "Tinygrad rozkłada skomplikowane obliczenia na **trzy podstawowe typy operacji**: elementarne, redukujące oraz przesunięcia danych. Wszystkie wyższe funkcje są zbudowane z kompozycji tych prostych bloków.\n",
    "\n",
    "Obliczenia są wykonywane **leniwie**, co pozwala na fuzję wielu operacji w jeden program GPU/CPU. Dla przyspieszenia powtarzalnych fragmentów kodu, oferuje prosty JIT (dekorator `@TinyJit`).\n",
    "\n",
    "Korzysta z prostych backendów: CPU realizuje operacje przez NumPy/Python, a GPU – przez OpenCL, CUDA, METAL itp. Dzięki temu wspiera wiele akceleratorów bez złożonej implementacji.\n",
    "\n",
    "## Praktyczne zastosowanie – Klasyfikator MNIST\n",
    "\n",
    "Poniżej kompletny przykład trenowania klasyfikatora MNIST w tinygrad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccf20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n",
      "(60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n",
      "acc 0.0576\n",
      "acc 0.5398\n",
      "acc 0.7032\n",
      "acc 0.6464\n",
      "acc 0.7536\n",
      "acc 0.7588\n",
      "acc 0.7260\n",
      "acc 0.7829\n",
      "acc 0.8066\n",
      "acc 0.8142\n",
      "total time 14.910480499267578\n"
     ]
    }
   ],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "from tinygrad.nn.datasets import mnist\n",
    "from time import time\n",
    "\n",
    "\n",
    "class MNISTClassifier:\n",
    "  def __init__(self):\n",
    "    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n",
    "    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n",
    "    self.l3 = nn.Linear(1600, 10)\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor:\n",
    "    x = self.l1(x).relu().max_pool2d((2,2))\n",
    "    x = self.l2(x).relu().max_pool2d((2,2))\n",
    "    return self.l3(x.flatten(1).dropout(0.5))\n",
    "\n",
    "print(Device.DEFAULT)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n",
    "\n",
    "model = MNISTClassifier()\n",
    "acc = (model(X_test).argmax(axis=1) == Y_test).mean()\n",
    "# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\n",
    "\n",
    "optim = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "batch_size = 64\n",
    "\n",
    "@TinyJit\n",
    "def step():\n",
    "  Tensor.training = True  # makes dropout work\n",
    "  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "  X, Y = X_train[samples], Y_train[samples]\n",
    "  optim.zero_grad()\n",
    "  loss = model(X).sparse_categorical_crossentropy(Y).backward()\n",
    "  optim.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "  loss = step()\n",
    "  if i%100 == 0:\n",
    "    Tensor.training = False\n",
    "    acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n",
    "    print(f\"acc {acc:.4f}\")\n",
    "print(\"total time\", time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([60000, 1, 28, 28]) torch.float32 torch.Size([60000]) torch.int64\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 826.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 300.44 MiB is free. Including non-PyTorch memory, this process has 2.41 GiB memory in use. Of the allocated memory 872.79 MiB is allocated by PyTorch, and 5.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m     acc \u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m Y_test\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/tinygrad-prezentacja/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tinygrad-prezentacja/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m, in \u001b[0;36mMNISTClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2(x)), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/tinygrad-prezentacja/venv/lib/python3.10/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 826.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 300.44 MiB is free. Including non-PyTorch memory, this process has 2.41 GiB memory in use. Of the allocated memory 872.79 MiB is allocated by PyTorch, and 5.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from time import time\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.l1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.l2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.l3 = nn.Linear(1600, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.l1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.l2(x)), (2, 2))\n",
    "        x = x.flatten(1)\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        return self.l3(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = torch.stack([img for img, _ in train_dataset])\n",
    "Y_train = torch.tensor([label for _, label in train_dataset])\n",
    "X_test = torch.stack([img for img, _ in test_dataset])\n",
    "Y_test = torch.tensor([label for _, label in test_dataset])\n",
    "\n",
    "print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n",
    "\n",
    "model = MNISTClassifier().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time()\n",
    "batch_size = 64\n",
    "for step in range(1000):\n",
    "    indices = torch.randint(0, len(X_train), (batch_size,))\n",
    "    X, Y = X_train[indices].to(device), Y_train[indices].to(device)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = F.cross_entropy(outputs, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test.to(device))\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            acc = (predicted == Y_test.to(device)).float().mean().item()\n",
    "        print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100:.2f}%\")\n",
    "print(\"total time\", time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f49c4c",
   "metadata": {},
   "source": [
    "## Zastosowania w projektach rzeczywistych\n",
    "\n",
    "Przykłady wykorzystania tinygrad w praktyce:\n",
    "\n",
    "- OpenPilot (komponent samojezdnego samochodu)\n",
    "- Mobilne przetwarzanie obrazów\n",
    "- Projekty edukacyjne wymagające zrozumienia całego stosu ML\n",
    "\n",
    "## Najważniejsze różnice z kulturą PyTorch i TensorFlow\n",
    "\n",
    "PyTorch/TensorFlow promują używanie gotowych abstrakcji wysokiego poziomu (modele, warstwy). tinygrad zachęca do budowania wszystkiego od podstaw - rozumienie, a nie tylko użycie. Różnica filozoficzna to również podejście do debugowania - w tinygrad widać wszystkie operacje, podczas gdy w większych bibliotekach wiele operacji dzieje się \"pod maską\".\n",
    "\n",
    "W tinygrad możemy użyć `with Context(DEBUG=2)`, aby zobaczyć wszystkie kernele, które są wykonywane. DEBUG=4, pokazuje cały kod, w PyTorch jest to ciężkie.\n",
    "\n",
    "## Przyszłość tinygrad\n",
    "\n",
    "Tinycorp regularnie rozwija projekt, dodając:\n",
    "\n",
    "- Wsparcie dla nowych architektur (Apple M1/M2)\n",
    "- Optymalizacje kompilatora (JIT)\n",
    "- Wsparcie dla nowych modeli (np. LLM, stable diffusion)\n",
    "\n",
    "## Zasoby do dalszego zgłębienia\n",
    "\n",
    "- Oficjalne repozytorium: https://github.com/tinygrad/tinygrad\n",
    "- Kanał YouTube George'a Hotza: omówienia implementacji https://www.youtube.com/@geohotarchive/videos\n",
    "- Dokumentacja: https://docs.tinygrad.org/\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "tinygrad to **bardzo lekki** framework, którego głównym celem jest edukacja i zrozumienie działania głębokiego uczenia. Oferuje uproszczony interfejs podobny do PyTorch, ale rezygnuje z wielu abstrakcji. Najważniejsze zalety: prostota implementacji, czytelność kodu i łatwość rozbudowy. Stanowi interesującą alternatywę do nauki mechanizmów uczenia, pokazując „od podszewki\" jak działa propagacja wsteczna i przetwarzanie tensorów.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
